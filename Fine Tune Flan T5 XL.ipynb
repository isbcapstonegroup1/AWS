{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26efc977",
   "metadata": {},
   "source": [
    "# Set AWS Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64380f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Get current region, role, and default bucket\n",
    "aws_region = boto3.Session().region_name  # Get the current AWS region\n",
    "aws_role = sagemaker.session.Session().get_caller_identity_arn()  # Get the ARN of the caller's identity\n",
    "output_bucket = sagemaker.Session().default_bucket()  # Get the default S3 bucket for SageMaker output\n",
    "\n",
    "# Define ANSI escape codes for formatting in the console\n",
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "# Print AWS region, role, and output bucket information\n",
    "print(f\"{bold}aws_region:{unbold} {aws_region}\")  # Display AWS region\n",
    "print(f\"{bold}aws_role:{unbold} {aws_role}\")  # Display AWS role\n",
    "print(f\"{bold}output_bucket:{unbold} {output_bucket}\")  # Display output S3 bucket\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b0176",
   "metadata": {},
   "source": [
    "# Select Model: Flan T5 XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d6d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import IPython\n",
    "from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.filters import And\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "\n",
    "# Default model choice\n",
    "model_id = \"huggingface-text2text-flan-t5-xl\"  # Set the default model ID\n",
    "\n",
    "# Identify FLAN T5 models that support fine-tuning using a filter\n",
    "filter_value = And(\n",
    "    \"task == text2text\", \"framework == huggingface\", \"training_supported == true\"\n",
    ")\n",
    "\n",
    "# List available FLAN T5 models for fine-tuning\n",
    "model_list = [m for m in list_jumpstart_models(filter=filter_value) if \"flan-t5\" in m]\n",
    "\n",
    "# Display the model IDs in a dropdown for user selection\n",
    "dropdown = Dropdown(\n",
    "    value=model_id,\n",
    "    options=model_list,\n",
    "    description=\"FLAN T5 models available for fine-tuning:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "\n",
    "# Display a markdown heading and the dropdown for user interaction\n",
    "display(IPython.display.Markdown(\"### Select a pre-trained model from the dropdown below\"))\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea348769",
   "metadata": {},
   "source": [
    "# Select Instance for Training [ml.p3.16xlarge] & Inference [ml.g5.2xlarge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa2b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.instance_types import retrieve_default\n",
    "\n",
    "# Retrieve the selected model ID and set model version\n",
    "model_id, model_version = dropdown.value, \"1.*\"\n",
    "\n",
    "# Retrieve the default instance types for training and inference\n",
    "training_instance_type = retrieve_default(\n",
    "    model_id=model_id, model_version=model_version, scope=\"training\"\n",
    ")\n",
    "inference_instance_type = retrieve_default(\n",
    "    model_id=model_id, model_version=model_version, scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Display the selected model ID and default instance types\n",
    "print(f\"{bold}model_id:{unbold} {model_id}\")  # Display selected model ID\n",
    "print(f\"{bold}training_instance_type:{unbold} {training_instance_type}\")  # Display training instance type\n",
    "print(f\"{bold}inference_instance_type:{unbold} {inference_instance_type}\")  # Display inference instance type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from SageMaker\n",
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "\n",
    "# Training instance will use this image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=aws_region,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"training\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "# Pre-trained model\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"training\"\n",
    ")\n",
    "\n",
    "# Script to execute on the training instance\n",
    "train_script_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"training\"\n",
    ")\n",
    "\n",
    "# Print the retrieved URIs\n",
    "print(f\"{bold}image uri:{unbold} {train_image_uri}\")\n",
    "print(f\"{bold}model uri:{unbold} {train_model_uri}\")\n",
    "print(f\"{bold}script uri:{unbold} {train_script_uri}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a8fef3",
   "metadata": {},
   "source": [
    "# Prepare Traing Data: Get Base Data and Add Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9ec843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
    "import numpy as np  # For numerical operations\n",
    "from io import StringIO  # For handling string input/output operations\n",
    "import json  # For working with JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display option to show complete column values without truncation\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b12f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Boto3 S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Specify your S3 bucket name and file key\n",
    "bucket_name = 'traindatanew'\n",
    "file_key = 'holcim_fine-tuning_data.csv'  # Get file name\n",
    "\n",
    "# Get object from S3\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "\n",
    "# Read the content of the file as a Pandas DataFrame\n",
    "content = response['Body'].read()\n",
    "data = pd.read_csv(StringIO(content.decode('utf-8')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt\n",
    "prompt_hanz_sales='''The schema of the table is as below in {} brackets.\n",
    "              {\"table\": \"hanz_sales\",\n",
    "               \"table_description\": \"It is designed to store daily sales data of different cement materials in New Zealand. The table contains several key columns, including 'Material' for various types of cement material codes, 'Ship-to' for customer codes, 'Material-Description' for detailed material descriptions, and 'Ship-to party' for customer descriptions. Additionally, it tracks important transaction information such as 'Delivery Date' (Del.Date), 'Reported Quantity' (Rpt Qty), and the unit of measurement for quantity, indicated by 'Quantity Unit in Tons' (Reporting UOM). This schema provides essential insights into the sales activities, allowing for a comprehensive analysis of cement material sales in the specified region.\",\n",
    "              \"columns\": [\n",
    "                               [\"Material\": \"The various types of cement material type code\"],\n",
    "                               [\"Ship-to\": \"The customer Codes\"],\n",
    "                               [\"Material-Description\": \"Material Description\"],\n",
    "                               [\"Ship-to party\": \"Customer description\"],\n",
    "                               [\"Del.Date\": \"Delivery Date\"],\n",
    "                               [\"Rpt Qty\": \"Reported Quantity]\",\n",
    "                               [\"Reporting UOM\": \"contains value T where Quantity of Unit is Tons\"]\n",
    "                        ]\n",
    "              }\n",
    "              Question 1:  What is the total reported quantity for the material '520110'?\n",
    "              Answer 1: The query should be \"SELECT SUM(\"Rpt Qty\") AS total_reported_quantity FROM hanz_sales WHERE \"Material\" = '520110';\"\n",
    "              Question 2: How many unique ship-to parties are there in the sales data? \n",
    "              Answer 2: The query should be \"SELECT COUNT(*) AS order_count FROM hanz_sales WHERE \"Del.Date\" = '2022-01-05';\"\n",
    "              Question 3: What is the total reported quantity for orders delivered in January 2022? \n",
    "              Answer 3: SELECT SUM(\"Rpt Qty\") AS total_reported_quantity FROM hanz_sales WHERE strftime('%Y', \"Del.Date\") = '2022' AND strftime('%m', \"Del.Date\") = '01'; \n",
    "              Question 4: How many orders were delivered for each reporting unit of measure (UOM)?\n",
    "              Answer 4: SELECT \"Reporting UOM\", COUNT(*) AS order_count FROM hanz_sales GROUP BY \"Reporting UOM\"; \n",
    "              Question 5: How many orders were delivered for each ship-to party where the delivery date is in January 2022 and the reported quantity is greater than 10 tons? \n",
    "              Answer 5: SELECT \"Ship-to party\", COUNT(*) AS order_count FROM hanz_sales WHERE strftime('%Y', \"Del.Date\") = '2022' AND strftime('%m', \"Del.Date\") = '01' AND \"Rpt Qty\" > 10 GROUP BY \"Ship-to party\"; \n",
    "              Question 6:  What is the total reported quantity for each ship-to party where the reported quantity is less than 1 ton?  \n",
    "              Answer 6: SELECT \"Ship-to party\", SUM(\"Rpt Qty\") AS total_reported_quantity FROM hanz_sales WHERE \"Rpt Qty\" [less than] 1 GROUP BY \"Ship-to party\"; \n",
    "              Question 7: What is the total reported quantity for each ship-to party where the reported quantity is less than and equal to 1 ton?  \n",
    "              Answer 7: SELECT \"Ship-to party\", SUM(\"Rpt Qty\") AS total_reported_quantity FROM hanz_sales WHERE \"Rpt Qty\" [less than equal to] 1 GROUP BY \"Ship-to party\"; \n",
    "              In a Similar way answer generate a SQL query to '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prompt_au_rmx='''The schema of the table is as below in {} brackets.\n",
    "              {\"table\": \"au_rmx\",\n",
    "               \"table_description\": \"Table containing billing, customer, material, and sales information, including details such as billing document number, billing date, customer code and name, material description, quantity ordered, plant manufactured, sales value, freight details, account manager, sales office information, and important dates like delivery and order creation.\",\n",
    "              \"columns\": [\n",
    "                              [\"billing_document\": \"billing /invoice document number\"],\n",
    "                               [\"billing_date\": \"date when the order was billed\"],\n",
    "                               [\"ship_to\": \"customer code\"],\n",
    "                               [\"ship_to_name\": \"customer name\"],\n",
    "                               [\"ship_to_region\": \"customer region\"],\n",
    "                               [\"material\": \"material brought by the customer]\",\n",
    "                               [\"quantity\": \"quantity ordered\"],\n",
    "                               [\"plant\": \"plant manufactured\"],\n",
    "                               [\"plant_name\": \"plant name\"],\n",
    "                               [\"plant_region\": \"plant region\"],\n",
    "                               [\"sales_net_value\": \"sales value of the material\"],\n",
    "                               [\"freight_net_value\": \"freight value of the material\"],\n",
    "                               [\"freight_revenue\": \"freight revenue of the material\"],\n",
    "                               [\"material_description\": \"material description\"],\n",
    "                               [\"cost\": \"cost of the material\"],\n",
    "                               [\"account_manager\": \"account manager\"],\n",
    "                               [\"sales_office\": \"sales office id\"],\n",
    "                               [\"sales_office_desc\": \"sales office name\"],\n",
    "                               [\"del_date\": \"delivery date\"],\n",
    "                               [\"ord_create\": \"order date\"]\n",
    "                        ]\n",
    "              }\n",
    "              Question 1: What is the total sales value for the orders billed on January 6, 2025?\n",
    "              Answer 1: SELECT SUM(sales_net_value) AS total_sales_value FROM au_rmx WHERE billing_date = '2025-01-06';\n",
    "              Question 2: Which orders have a delivery date in the year 2025?\n",
    "              Answer 2: SELECT * FROM au_rmx WHERE strftime('%Y', del_date) = '2025'; \n",
    "              Question 3: Which orders have a billing date in January 2025 and a delivery date in February 2025? \n",
    "              Answer 3: SELECT * FROM au_rmx WHERE strftime('%Y', billing_date) = '2025' AND strftime('%m', billing_date) = '01' AND strftime('%Y', del_date) = '2025' AND strftime('%m', del_date) = '02'; \n",
    "              Question 4: Which sales office had the highest total sales value? \n",
    "              Answer 4: SELECT sales_office_desc FROM au_rmx GROUP BY sales_office_desc ORDER BY SUM(sales_net_value) DESC LIMIT 1; \n",
    "              Question 5: What is the total quantity of material ordered by each customer? \n",
    "              Answer 5: SELECT ship_to_name, SUM(quantity) AS total_quantity_ordered FROM au_rmx GROUP BY ship_to_name;\n",
    "              Question 6: How many orders have a delivery date before their order date or same as order? \n",
    "              Answer 6: SELECT COUNT(*) AS order_count FROM au_rmx WHERE del_date [less than equal to] ord_create;\n",
    "             Question 7: How many orders have a delivery date before their order date? \n",
    "              Answer 7: SELECT COUNT(*) AS order_count FROM au_rmx WHERE del_date [less than] ord_create;\n",
    "              In a Similar way answer generate a SQL query to '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecefa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'context' based on conditions\n",
    "data['context'] = np.where(\n",
    "    data['table_name'] == 'hanz_sales',  # Check if 'table_name' is 'hanz_sales'\n",
    "    prompt_hanz_sales + data['question'],  # If true, concatenate 'prompt_hanz_sales' with 'question'\n",
    "    np.where(\n",
    "        data['table_name'] == 'au_rmx',  # Check if 'table_name' is 'au_rmx'\n",
    "        prompt_au_rmx + data['question'],  # If true, concatenate 'prompt_au_rmx' with 'question'\n",
    "        None  # If neither condition is true, set 'context' to None\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the first 10 rows of the modified DataFrame\n",
    "data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee01ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the DataFrame into train and test sets with a 35% test size and a random seed for reproducibility\n",
    "#train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train, test = train_test_split(data, test_size=0.35, random_state=42)\n",
    "\n",
    "# Creating copies of the subsets to ensure they are separate DataFrame objects\n",
    "train = train.copy()  # Copy the train set\n",
    "# train = train.head(5)  # Optional: Uncomment this line to take only the first 5 rows of the train set\n",
    "test = test.copy()  # Copy the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3ef878",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d572a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf937f",
   "metadata": {},
   "source": [
    "# write test.csv in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba492546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIO buffer to store the CSV data\n",
    "csv_buffer = StringIO()\n",
    "\n",
    "# Write the test DataFrame to the CSV buffer, excluding the index column\n",
    "test.to_csv(csv_buffer, index=False)\n",
    "\n",
    "# Upload the CSV string to S3\n",
    "s3_client.put_object(Body=csv_buffer.getvalue(), Bucket=bucket_name, Key='test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d1d858",
   "metadata": {},
   "source": [
    "# Write data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54751948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIO buffer to store the CSV data\n",
    "csv_buffer = StringIO()\n",
    "\n",
    "# Write the entire DataFrame to the CSV buffer, excluding the index column\n",
    "data.to_csv(csv_buffer, index=False)\n",
    "\n",
    "# Upload the CSV string to S3\n",
    "s3_client.put_object(Body=csv_buffer.getvalue(), Bucket=bucket_name, Key='data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b9bca2",
   "metadata": {},
   "source": [
    "# Write train.jsonl in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7feb370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sagemaker.s3 import S3Uploader\n",
    "import json\n",
    "\n",
    "# Function to convert DataFrame to a list of dictionaries in JSON Lines format\n",
    "def df_to_jsonl(df, context, sql):\n",
    "    # Use the apply function to iterate through rows and create dictionaries\n",
    "    jsonl_data = df.apply(lambda row: json.dumps({\"prompt\": row[context], \"completion\": row[sql]}), axis=1).tolist()\n",
    "    return jsonl_data\n",
    "\n",
    "# Define column names for context and SQL in the DataFrame\n",
    "context = 'context'\n",
    "sql = 'sql'\n",
    "\n",
    "# Call the function to convert the 'train' DataFrame to a list of dictionaries in JSON Lines format\n",
    "train_jsonl = df_to_jsonl(train, context, sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1501f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the list of dictionaries to a JSON Lines file locally\n",
    "with open('train.jsonl', 'w') as f:\n",
    "    # Iterate through the list and write each dictionary as a JSON Lines record to the file\n",
    "    for item in train_jsonl:\n",
    "        f.write(f\"{str(item)}\\n\")\n",
    "\n",
    "# AWS credentials and S3 bucket information\n",
    "bucket_name = 'your_bucket_name'  # Replace 'your_bucket_name' with the actual S3 bucket name\n",
    "s3_key = 'train.jsonl'  # Specify the desired path in the S3 bucket\n",
    "\n",
    "# Create an S3 client (uncomment the following lines if using boto3)\n",
    "# import boto3\n",
    "# s3_client = boto3.client('s3', aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n",
    "\n",
    "# Upload the local JSON Lines file to S3\n",
    "# Note: Make sure to uncomment and provide AWS credentials if not already configured\n",
    "s3_client.upload_file('train.jsonl', bucket_name, s3_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e767e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials and S3 bucket information\n",
    "bucket_name = output_bucket  # Replace 'output_bucket' with your actual S3 bucket name\n",
    "s3_key = 'train_data/train.jsonl'  # Specify the desired path in the S3 bucket\n",
    "\n",
    "# Upload the local JSON Lines file to S3\n",
    "# Note: Uncomment the following line if 's3' is uncommented above and 'train.jsonl' is the local file to be uploaded\n",
    "s3_client.upload_file('train.jsonl', bucket_name, s3_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaad0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the S3 location for training data\n",
    "train_data_location = f\"s3://{output_bucket}/train_data\"  # Replace 'output_bucket' with your actual S3 bucket name\n",
    "\n",
    "# Print the S3 location for reference\n",
    "print(train_data_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary module for hyperparameters in SageMaker\n",
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "# Note: Replace 'model_id' and 'model_version' with your actual model ID and version\n",
    "hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d3f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override some default hyperparameters with custom values\n",
    "hyperparameters[\"epochs\"] = \"3\"\n",
    "hyperparameters[\"max_input_length\"] = \"1024\"  # Data inputs will be truncated at this length\n",
    "hyperparameters[\"max_output_length\"] = \"128\"  # Data outputs will be truncated at this length\n",
    "hyperparameters[\"batch_size\"] = '8'\n",
    "hyperparameters[\"learning_rate\"] = '0.00001'\n",
    "hyperparameters[\"max_grad_norm\"] = '5.0'\n",
    "hyperparameters[\"peft_type\"] = 'lora'\n",
    "hyperparameters[\"gradient_accumulation_steps\"] = '4'\n",
    "\n",
    "# Print the updated hyperparameters for reference\n",
    "print(hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3dcf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules for SageMaker Estimator\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "# Extract the most informative part of the model ID for better readability\n",
    "model_name = \"-\".join(model_id.split(\"-\")[2:])\n",
    "\n",
    "# Generate a training job name based on the model name and hyperparameter settings\n",
    "training_job_name = name_from_base(f\"js-demo-{model_name}-{hyperparameters['epochs']}\")\n",
    "\n",
    "# Print the generated training job name for reference\n",
    "print(f\"{bold}job name:{unbold} {training_job_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location='s3://finetune-model/bycode/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb99c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training metric definitions to capture key metrics during training.\n",
    "training_metric_definitions = [\n",
    "    {\"Name\": \"val_loss\", \"Regex\": \"'eval_loss': ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"train_loss\", \"Regex\": \"'loss': ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"epoch\", \"Regex\": \"'epoch': ([0-9\\\\.]+)\"},\n",
    "]\n",
    "\n",
    "# Create SageMaker Estimator instance with specified configurations.\n",
    "sm_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    source_dir=train_script_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    volume_size=300,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=output_location,\n",
    "    metric_definitions=training_metric_definitions,  # Attach metric definitions for tracking training metrics.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce777ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch a SageMaker training job over data located in the given S3 path\n",
    "# Training jobs can take hours, it is recommended to set wait=False,\n",
    "# and monitor job status through SageMaker console\n",
    "sm_estimator.fit({\"training\": train_data_location}, job_name=training_job_name, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a283893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "# This can be called while the job is still running\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbbe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "# Generate a name for the fine-tuned model based on the model_id\n",
    "fine_tuned_name = name_from_base(f\"jumpstart-demo-fine-tuned-{model_id}\")\n",
    "\n",
    "# Construct the URI for the fine-tuned model using the output location and training job name\n",
    "fine_tuned_model_uri = f\"{output_location}{training_job_name}/output/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da584bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58478e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "# Retrieve the inference docker image URI using SageMaker image_uris module.\n",
    "# The base HuggingFace container image is automatically inferred from the provided model_id.\n",
    "# The image is scoped for inference, and the instance type for deployment is specified.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,  # Region is automatically inferred from the SageMaker session\n",
    "    framework=None,  # Framework is automatically inferred from model_id\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"inference\",  # Set the image scope for inference\n",
    "    instance_type=inference_instance_type,  # Specify the instance type for deployment\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21759c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SageMaker model instance for the fine-tuned model.\n",
    "# The model is configured with the deployment image URI, fine-tuned model URI,\n",
    "# AWS role, custom predictor class, and a specified name.\n",
    "fine_tuned_model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=fine_tuned_model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,  # Custom predictor class for the model\n",
    "    name=fine_tuned_name,  # Name for the SageMaker model instance\n",
    ")\n",
    "\n",
    "# Print the image URI and model URI for reference.\n",
    "print(f\"{bold}image URI:{unbold}{newline} {deploy_image_uri}\")\n",
    "print(f\"{bold}model URI:{unbold}{newline} {fine_tuned_model_uri}\")\n",
    "\n",
    "# Indicate the start of the deployment process.\n",
    "print(\"Deploying an endpoint ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the fine-tuned model using the SageMaker deploy method.\n",
    "# The deployment is configured with an initial instance count, instance type,\n",
    "# custom predictor class, and a specified endpoint name.\n",
    "fine_tuned_predictor = fine_tuned_model.deploy(\n",
    "    initial_instance_count=1,  # Number of instances to deploy initially\n",
    "    instance_type=inference_instance_type,  # Type of instance for deployment\n",
    "    predictor_cls=Predictor,  # Custom predictor class for the deployed model\n",
    "    endpoint_name=fine_tuned_name,  # Specify the name for the deployed endpoint\n",
    ")\n",
    "\n",
    "# Print a message indicating the successful deployment.\n",
    "print(f\"{newline}Deployed an endpoint {fine_tuned_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aafa5e",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8947e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Boto3 S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Specify your S3 bucket name and file key\n",
    "bucket_name = 'sagemaker-ap-southeast-1-789372484921'\n",
    "# Get file name\n",
    "file_key = 'data.csv'\n",
    "\n",
    "# Get object from S3\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "\n",
    "# Read the content of the file as a Pandas DataFrame\n",
    "content = response['Body'].read()\n",
    "read_data = pd.read_csv(StringIO(content.decode('utf-8')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c965d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read_data.shape)\n",
    "read_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024085c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Parameters for (output) text generation. For a comprehensive introduction to generation\n",
    "# parameters, refer to https://huggingface.co/blog/how-to-generate\n",
    "parameters = {\n",
    "    \"max_length\": 128,      # Restrict the length of the generated text\n",
    "    # \"num_return_sequences\": 5,  # We will inspect several model outputs\n",
    "    \"num_beams\": 10         # Use beam search\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ff7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for running inference queries\n",
    "def query_endpoint_with_json_payload(payload, endpoint_name):\n",
    "    encoded_json = json.dumps(payload).encode(\"utf-8\")\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd644c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response_multiple_texts(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[\"generated_texts\"]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a24cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The schema of the table is as below in {} brackets.\n",
    "              {\"table\": \"au_rmx\",\n",
    "               \"table_description\": \"Table containing billing, customer, material, and sales information, including details such as billing document number, billing date, customer code and name, material description, quantity ordered, plant manufactured, sales value, freight details, account manager, sales office information, and important dates like delivery and order creation.\",\n",
    "              \"columns\": [\n",
    "                              [\"billing_document\": \"billing /invoice document number\"],\n",
    "                               [\"billing_date\": \"date when the order was billed\"],\n",
    "                               [\"ship_to\": \"customer code\"],\n",
    "                               [\"ship_to_name\": \"customer name\"],\n",
    "                               [\"ship_to_region\": \"customer region\"],\n",
    "                               [\"material\": \"material brought by the customer]\",\n",
    "                               [\"quantity\": \"quantity ordered\"],\n",
    "                               [\"plant\": \"plant manufactured\"],\n",
    "                               [\"plant_name\": \"plant name\"],\n",
    "                               [\"plant_region\": \"plant region\"],\n",
    "                               [\"sales_net_value\": \"sales value of the material\"],\n",
    "                               [\"freight_net_value\": \"freight value of the material\"],\n",
    "                               [\"freight_revenue\": \"freight revenue of the material\"],\n",
    "                               [\"material_description\": \"material description\"],\n",
    "                               [\"cost\": \"cost of the material\"],\n",
    "                               [\"account_manager\": \"account manager\"],\n",
    "                               [\"sales_office\": \"sales office id\"],\n",
    "                               [\"sales_office_desc\": \"sales office name\"],\n",
    "                               [\"del_date\": \"delivery date\"],\n",
    "                               [\"ord_create\": \"order date\"]\n",
    "                        ]\n",
    "              }\n",
    "              Question 1: What is the total sales value for the orders billed on January 6, 2025?\n",
    "              Answer 1: SELECT SUM(sales_net_value) AS total_sales_value FROM au_rmx WHERE billing_date = '2025-01-06';\n",
    "              Question 2: Which orders have a delivery date in the year 2025?\n",
    "              Answer 2: SELECT * FROM au_rmx WHERE strftime('%Y', del_date) = '2025'; \n",
    "              Question 3: Which orders have a billing date in January 2025 and a delivery date in February 2025? \n",
    "              Answer 3: SELECT * FROM au_rmx WHERE strftime('%Y', billing_date) = '2025' AND strftime('%m', billing_date) = '01' AND strftime('%Y', del_date) = '2025' AND strftime('%m', del_date) = '02'; \n",
    "              Question 4: Which sales office had the highest total sales value? \n",
    "              Answer 4: SELECT sales_office_desc FROM au_rmx GROUP BY sales_office_desc ORDER BY SUM(sales_net_value) DESC LIMIT 1; \n",
    "              Question 5: What is the total quantity of material ordered by each customer? \n",
    "              Answer 5: SELECT ship_to_name, SUM(quantity) AS total_quantity_ordered FROM au_rmx GROUP BY ship_to_name;\n",
    "              Question 6: How many orders have a delivery date before their order date or same as order? \n",
    "              Answer 6: SELECT COUNT(*) AS order_count FROM au_rmx WHERE del_date [less than equal to] ord_create;\n",
    "             Question 7: How many orders have a delivery date before their order date? \n",
    "              Answer 7: SELECT COUNT(*) AS order_count FROM au_rmx WHERE del_date [less than] ord_create;\n",
    "              In a Similar way answer generate a SQL query to 'What is the average sales net value for orders delivered in each region?'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75192574",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"{context}\"\n",
    "def generate_questions(endpoint_name, text):\n",
    "    expanded_prompt = prompt.replace(\"{context}\", text)\n",
    "    payload = {\"text_inputs\": expanded_prompt, **parameters}\n",
    "    query_response = query_endpoint_with_json_payload(payload, endpoint_name=endpoint_name)\n",
    "    generated_texts = parse_response_multiple_texts(query_response)\n",
    "#     for i, generated_text in enumerate(generated_texts):\n",
    "#         print(f\"Response {i}: {generated_text}{newline}\")\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44e19a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=generate_questions(fine_tuned_name, text)[0]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fda9ab",
   "metadata": {},
   "source": [
    "# pred 330 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data['pred_sql']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4950cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(read_data)):\n",
    "    print(i)\n",
    "    x=generate_questions(fine_tuned_name, read_data['context'][i])\n",
    "    read_data['pred_sql'][i]=x[0]\n",
    "    print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078efd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_buffer = StringIO()\n",
    "read_data.to_csv(csv_buffer,index=False)\n",
    "# Upload the CSV string to S3\n",
    "s3_client.put_object(Body=csv_buffer.getvalue(), Bucket=bucket_name, Key='read_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c9bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete resources\n",
    "pre_trained_predictor.delete_model()\n",
    "pre_trained_predictor.delete_endpoint()\n",
    "fine_tuned_predictor.delete_model()\n",
    "fine_tuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f88f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2722dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
